<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="one-fits-all-power-general-time-series-analysis-by-pretrained-lm">One Fits All: Power General Time Series Analysis by Pretrained LM</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>略</p> <h2 id="2-创新点">2. 创新点</h2> <p>任务全面，方法虽然相对简单，但微调效果良好。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="./pic/onefitsall/structure.jpg" alt="方法概述图"> 本研究的方法简单直接：</p> <ul> <li> <strong>Self-attention</strong> 和 <strong>FFN</strong> 被冻结，即LLM的核心部分被冻结。只训练 <strong>positional embedding</strong>、<strong>input embedding</strong>、<strong>线性输出层</strong> 和 <strong>Layer Norm层</strong>。</li> <li>其中 <strong>positional embedding</strong> 和 <strong>layer norm</strong> 需要针对不同的下游任务进行训练，这是很自然的过程。</li> <li> <strong>Input embedding</strong> 是必须进行的步骤，这里利用的技术是 <strong>linear probing</strong>（参数较少）。</li> <li>进行简单的均值-方差归一化。</li> <li> <strong>Patching</strong>，即聚合相邻时间步来形成一个token，在同样的输入长度下，这样可以覆盖更大跨度的时间范围。</li> </ul> <h3 id="connecting-self-attention-with-pca">Connecting Self-Attention with PCA</h3> <p>作者在文章中还证明了 <strong>self-attention</strong> 和 <strong>PCA</strong> 在作用上的相似性，具体证明省略。从实验来看，二者在功能上显示出一定的相似性，这强调了预训练好的 self-attention 对于建模各种模态数据的通用性。</p> <h2 id="4-数据集">4. 数据集</h2> <p>使用了多种数据集进行了广泛的任务测试，详见原文。</p> <h2 id="5-实验结果">5. 实验结果</h2> <ul> <li>长期预测：预测长度更长</li> <li>短期预测：预测长度较短</li> <li>零样本预测：未进行微调</li> <li>少样本预测：只用极少量的训练样本进行微调</li> <li>分类</li> <li>异常检测</li> <li>插补</li> </ul> <p>以上七项任务均进行了测试。</p> <h2 id="6-实验环境">6. 实验环境</h2> <p>作者论文中详细列出了计算成本。</p> <h2 id="7-复现">7. 复现</h2> <p>FINISH</p> </body></html>