<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="time-llm-time-series-forecasting-by-reprogramming-large-language-models">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>LLM 在 NLP/CV 领域的表现非常优异，具有很强的通用性，在 zero-shot 和 few-shot 任务上也展现出了良好的性能。然而，在时序预测(ST)领域，大多数模型都具有较强的针对性，缺乏通用性。研究显示，LLM 在模式识别和复杂 token 序列理解方面具有良好的鲁棒性。</p> <h2 id="2-创新点">2. 创新点</h2> <p>本研究在时序预测领域首次尝试引入 LLM，采用的是一种轻量级方法，通过添加轻量级调节层，无需修改原有 LLM 参数，避免了昂贵的训练成本。这些调节层能够在少量样本上进行微调，适应当前任务。其中，时序特征的 reprogramming 是一个创新点，具有新颖性和可解释性。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="/pic/time-llm/structure.jpg" alt="架构图"> 架构中的 LLM 参数保持不变（Frozen），在其前后各添加了一个可训练的层（Patch Reprogramming 和 Output Projection）。这里采用通道独立策略。</p> <h3 id="input-embedding">Input Embedding</h3> <p>如上图序号 1 和序号 2 所示，时间序列先通过 RevIN 的归一化操作，然后分 patch 进行 embedding。具体数据格式可见： <img src="/pic/time-llm/data.jpg" alt="数据格式"></p> <h3 id="patch-reprogramming">Patch Reprogramming</h3> <p>数据通过前面的处理后仍为时序数据，因此需要转换为文本格式供 LLM 使用。这里采用了 cross-attention 来对齐不同模态，通过一个 linear 层从 V 个词中抽取 V’ 个 prototypes，减少了处理的复杂性。具体架构如下图所示： <img src="/pic/time-llm/reprogram.jpg" alt="架构细节"></p> <h3 id="prompt-as-prefix">Prompt-as-Prefix</h3> <p>将时间序列数据集的一些先验信息，以自然语言的形式作为前缀 prompt，并与对齐后的时序特征拼接后输入到 LLM。这可以参照总体架构图的序号 4。一个可能的 prompt 示例： <img src="/pic/time-llm/prompt.jpg" alt="Prompt 示例"></p> <h3 id="output-projection">Output Projection</h3> <p>丢弃前缀部分，将剩余部分扁平化处理，然后通过线性投影映射到最终结果的格式上。</p> <h2 id="4-数据集">4. 数据集</h2> <ul> <li>长期：ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI</li> <li>短期：M4 benchmark</li> </ul> <h2 id="5-实验结果">5. 实验结果</h2> <ul> <li>长期、短期</li> <li>Few-shot on 10%/5%</li> <li>Zero-shot</li> </ul> <p>整个效果都不错，具体数据可见原论文的Exp部分，这里省略。</p> <h2 id="6-复现情况">6. 复现情况</h2> <p>FINISH</p> </body></html>