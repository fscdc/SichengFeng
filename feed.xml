<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T09:27:24+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-20-time-llm/" rel="alternate" type="text/html" title=""/><published>2024-09-02T09:27:24+00:00</published><updated>2024-09-02T09:27:24+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-20-time-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-20-time-llm/"><![CDATA[<h1 id="time-llm-time-series-forecasting-by-reprogramming-large-language-models">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>LLM 在 NLP/CV 领域的表现非常优异，具有很强的通用性，在 zero-shot 和 few-shot 任务上也展现出了良好的性能。然而，在时序预测(ST)领域，大多数模型都具有较强的针对性，缺乏通用性。研究显示，LLM 在模式识别和复杂 token 序列理解方面具有良好的鲁棒性。</p> <h2 id="2-创新点">2. 创新点</h2> <p>本研究在时序预测领域首次尝试引入 LLM，采用的是一种轻量级方法，通过添加轻量级调节层，无需修改原有 LLM 参数，避免了昂贵的训练成本。这些调节层能够在少量样本上进行微调，适应当前任务。其中，时序特征的 reprogramming 是一个创新点，具有新颖性和可解释性。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="/pic/time-llm/structure.jpg" alt="架构图"/> 架构中的 LLM 参数保持不变（Frozen），在其前后各添加了一个可训练的层（Patch Reprogramming 和 Output Projection）。这里采用通道独立策略。</p> <h3 id="input-embedding">Input Embedding</h3> <p>如上图序号 1 和序号 2 所示，时间序列先通过 RevIN 的归一化操作，然后分 patch 进行 embedding。具体数据格式可见： <img src="/pic/time-llm/data.jpg" alt="数据格式"/></p> <h3 id="patch-reprogramming">Patch Reprogramming</h3> <p>数据通过前面的处理后仍为时序数据，因此需要转换为文本格式供 LLM 使用。这里采用了 cross-attention 来对齐不同模态，通过一个 linear 层从 V 个词中抽取 V’ 个 prototypes，减少了处理的复杂性。具体架构如下图所示： <img src="/pic/time-llm/reprogram.jpg" alt="架构细节"/></p> <h3 id="prompt-as-prefix">Prompt-as-Prefix</h3> <p>将时间序列数据集的一些先验信息，以自然语言的形式作为前缀 prompt，并与对齐后的时序特征拼接后输入到 LLM。这可以参照总体架构图的序号 4。一个可能的 prompt 示例： <img src="/pic/time-llm/prompt.jpg" alt="Prompt 示例"/></p> <h3 id="output-projection">Output Projection</h3> <p>丢弃前缀部分，将剩余部分扁平化处理，然后通过线性投影映射到最终结果的格式上。</p> <h2 id="4-数据集">4. 数据集</h2> <ul> <li>长期：ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI</li> <li>短期：M4 benchmark</li> </ul> <h2 id="5-实验结果">5. 实验结果</h2> <ul> <li>长期、短期</li> <li>Few-shot on 10%/5%</li> <li>Zero-shot</li> </ul> <p>整个效果都不错，具体数据可见原论文的Exp部分，这里省略。</p> <h2 id="6-复现情况">6. 复现情况</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="./Backdoor/BL-survey.md">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="./Backdoor/servey4llmsp.md">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="./Backdoor/BA4llm.md">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="./bio/scGPT.md">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry><entry><title type="html">Efficient LLM</title><link href="https://fscdc.github.io/blog/2024/efficient-llm/" rel="alternate" type="text/html" title="Efficient LLM"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/efficient-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/efficient-llm/"><![CDATA[<p>This Page collects the papers and codes of Efficient AI, Efficient Large Language Models (LLMs). Additional, I read paper and take notes.</p> <p>LIST:</p> <h2 id="-efficient-llm">🦙 Efficient LLM</h2> <h3 id="survey">Survey</h3> <ul> <li>Efficient Large Language Models: A Survey, <a href="https://arxiv.org/abs/2312.03863">Arxiv</a>, <a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Github repo</a></li> </ul> <h2 id="books--courses">Books &amp; Courses</h2> <ul> <li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han (I may update some my learning notes later on my <a href="https://fscdc.github.io/">homepage</a>)</li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Efficient LLM]]></summary></entry><entry><title type="html">Interesting Paper</title><link href="https://fscdc.github.io/blog/2024/interesting/" rel="alternate" type="text/html" title="Interesting Paper"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/interesting</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/interesting/"><![CDATA[<p>This Page collects the papers and codes of which attracted my interests. Additional, I read paper and take notes.</p> <p><em>Keyword: effective LLM, multimodal, cross-discipline, Leverage Learning.</em></p> <p>LIST:</p> <ul> <li> <p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces, in <em>arXiv</em> 2023. <a href="https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf">[Paper]</a></p> </li> <li> <p>Token-Efficient Leverage Learning in Large Language Models, in <em>arXiv</em> 2024. <a href="https://arxiv.org/pdf/2404.00914.pdf">[Paper]</a></p> </li> <li> <p>Heterogeneous Graph Neural Network, in <em>ACM</em> 2019 <a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330961">[Paper]</a> <a href="./interesting/het.md">[Note]</a></p> </li> <li> <p>S4 model, <a href="https://arxiv.org/pdf/2111.00396.pdf">[Paper]</a> <a href="./interesting/s4.md">[Note]</a></p> </li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Interesting Paper]]></summary></entry><entry><title type="html">Model for Time Series</title><link href="https://fscdc.github.io/blog/2024/model4ts/" rel="alternate" type="text/html" title="Model for Time Series"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/model4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/model4ts/"><![CDATA[<p>This Page collects the papers and codes of Large Language Models (LLMs) and Foundation Models (FMs) for Time Series (TS). Additional, I read paper and take notes.</p> <p><em>After the success of BERT, GPT, and other LLMs in NLP, some researchers have proposed to apply LLMs to Time Series (TS) tasks. They fintune the LLMs on TS datasets and achieve SOTA results.</em></p> <p>LIST:</p> <h2 id="-llms-for-time-series">🦙 LLMs for Time Series</h2> <ul> <li> <p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. <a href="https://arxiv.org/abs/2310.01728">[Paper]</a> <a href="./LLM-TS/time-llm.md">[Note]</a></p> </li> <li> <p>TEST: Text Prototype Aligned Embedding to Activate LLM’s Ability for Time Series. <a href="https://arxiv.org/abs/2308.08241">[Paper]</a> <a href="./LLM-TS/test.md">[Note]</a></p> </li> <li> <p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting Hao, in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2210.08964">[Paper]</a> <a href="./LLM-TS/promptcast.md">[Note]</a></p> </li> <li> <p>One Fits All: Power General Time Series Analysis by Pretrained LM, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2302.11939">[Paper]</a> <a href="./LLM-TS/onefitsall.md">[Note]</a></p> </li> <li> <p>Temporal Data Meets LLM – Explainable Financial Time Series Forecasting, in <em>arXiv</em> 2023. <a href="https://arxiv.org/abs/2306.11025">[Paper]</a></p> </li> <li> <p>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. <a href="https://arxiv.org/abs/2308.08469">[Paper]</a> <a href="./LLM-TS/llm4ts.md">[Note]</a></p> </li> <li> <p>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. <a href="https://arxiv.org/abs/2309.06236">[Paper]</a></p> </li> <li> <p>Large Language Models Are Zero-Shot Time Series Forecasters. <a href="https://arxiv.org/abs/2310.07820">[Paper]</a> <a href="./LLM-TS/llm4zeroshot.md">[Note]</a></p> </li> <li> <p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. <a href="https://arxiv.org/abs/2310.04948">[Paper]</a> <a href="./LLM-TS/tempo.md">[Note]</a></p> </li> <li> <p>S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting. <a href="https://arxiv.org/pdf/2403.05798.pdf">[Paper]</a></p> </li> </ul> <h3 id="-survey">📍 Survey</h3> <ul> <li> <p>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. <a href="https://arxiv.org/abs/2310.10196">[Survey]</a></p> </li> <li> <p>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis. <a href="https://arxiv.org/abs/2402.02713">[Survey]</a></p> </li> <li> <p>Foundation Models for Time Series Analysis: A Tutorial and Survey <a href="https://arxiv.org/abs/2403.14735">[Survey]</a></p> </li> </ul> <h2 id="-related-fields">🔗 Related Fields</h2> <p><em>Here, some related fields are listed. These fields are not the main focus of this project, but they are also important for understanding how LLMs are applied to other fields rather than NLP and FMs in specific fields are developed.</em></p> <h3 id="-llm-for-recommendation-systems">📍 LLM for Recommendation Systems</h3> <ul> <li>Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5), in <em>arXiv</em> 2022. <a href="https://arxiv.org/abs/2203.13366">[Paper]</a></li> <li>LLM4Rec. <a href="https://github.com/WLiK/LLM4Rec">[GitHub]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Model for Time Series]]></summary></entry><entry><title type="html">Paper Notes</title><link href="https://fscdc.github.io/blog/2024/PaperNotes/" rel="alternate" type="text/html" title="Paper Notes"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://fscdc.github.io/blog/2024/PaperNotes</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/PaperNotes/"><![CDATA[<p>This post is for update my papers notes. Below are my research notes from papers in various fields:</p> <ol> <li>Please click <a href="https://fscdc.github.io/blog/2024/model4ts/">HERE</a>(LLM-based Time series analysis) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/backdoor/">HERE</a>(Backdoor attacks) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/interesting/">HERE</a>(Interesting papers) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/bio/">HERE</a>(papers about AI4Bio, scRNA-seq. etc.) for more details.</li> <li>Please click <a href="https://fscdc.github.io/blog/2024/efficient-llm/">HERE</a>(papers about Efficient AI, Efficient LLM, etc.) for more details. </li> <li>Please click <a href="">HERE</a>(papers about Pruning, not ready for now) for more details.</li> </ol> ]]></content><author><name></name></author><category term="summary"/><category term="note"/><summary type="html"><![CDATA[this is my notes for papers]]></summary></entry></feed>