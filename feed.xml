<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fscdc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fscdc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-llm4ts/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-llm4ts</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-llm4ts/"><![CDATA[<h1 id="llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms">LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs</h1> <p>这里我主要说明一下<strong>Method</strong>：</p> <p><img src="/pic/llm4ts/structure.jpg" alt="整体架构"/></p> <h2 id="time-series-alignment">Time-Series Alignment</h2> <p>对时间序列数据进行自回归处理，分析输入和输出之间的关系。</p> <h3 id="1-instance-normalization">1. Instance Normalization</h3> <p>针对整个数据实例进行归一化处理，是数据预处理的一部分。</p> <h3 id="2-time-series-tokenization">2. Time-Series Tokenization</h3> <p>通过通道独立策略加上patching处理，这是一种非常典型的数据处理方法。 <img src="/pic/llm4ts/patch.jpg" alt="时间序列Tokenization示意图"/></p> <h3 id="3-three-encodings-for-patched-time-series-data">3. Three Encodings for Patched Time-Series Data</h3> <ul> <li><strong>Token嵌入层</strong>：使用一维卷积基层作为token嵌入层。</li> <li><strong>位置层</strong>：使用可训练的查找表标识每个patch的位置。</li> <li><strong>时间嵌入层</strong>：实现双级聚合。 <ul> <li>首先，为每个时间属性（如秒、分钟等）使用一个可训练的查找表，将每个属性映射到一个高维空间中，然后将这些映射求和，形成一个统一的时间嵌入表示。</li> <li>每个patch可能包含多个时间戳，采用池化方法来提取最终的时间嵌入，通常使用片段中的第一个时间戳作为整个片段的代表。</li> </ul> </li> </ul> <h3 id="4-partial-freezing-and-tunable-layers">4. Partial Freezing and Tunable Layers</h3> <p>冻结模型的某些部分，同时保留部分层为可训练状态，以优化模型性能。</p> <ul> <li><strong>Layer Normalization Tuning</strong></li> <li><strong>Low-Rank Adaptation (LoRA)</strong></li> </ul> <h2 id="forecasting-fine-tuning-strategy">Forecasting Fine-Tuning Strategy</h2> <p>采用initial linear probing followed by full fine-tuning (LP-FT)策略，这一策略的优越性在于其双阶段方法：</p> <ul> <li>首先优化输出层，以最小化微调中的调整量（保留特征提取器在OOD（Out-Of-Distribution）场景中的有效性）。</li> <li>然后进行全面的微调，以使模型适应特定任务（提高ID（In-Distribution）精度）。</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-onefitsall/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-onefitsall</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-onefitsall/"><![CDATA[<h1 id="one-fits-all-power-general-time-series-analysis-by-pretrained-lm">One Fits All: Power General Time Series Analysis by Pretrained LM</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>略</p> <h2 id="2-创新点">2. 创新点</h2> <p>任务全面，方法虽然相对简单，但微调效果良好。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="/pic/onefitsall/structure.jpg" alt="方法概述图"/> 本研究的方法简单直接：</p> <ul> <li><strong>Self-attention</strong> 和 <strong>FFN</strong> 被冻结，即LLM的核心部分被冻结。只训练 <strong>positional embedding</strong>、<strong>input embedding</strong>、<strong>线性输出层</strong> 和 <strong>Layer Norm层</strong>。</li> <li>其中 <strong>positional embedding</strong> 和 <strong>layer norm</strong> 需要针对不同的下游任务进行训练，这是很自然的过程。</li> <li><strong>Input embedding</strong> 是必须进行的步骤，这里利用的技术是 <strong>linear probing</strong>（参数较少）。</li> <li>进行简单的均值-方差归一化。</li> <li><strong>Patching</strong>，即聚合相邻时间步来形成一个token，在同样的输入长度下，这样可以覆盖更大跨度的时间范围。</li> </ul> <h3 id="connecting-self-attention-with-pca">Connecting Self-Attention with PCA</h3> <p>作者在文章中还证明了 <strong>self-attention</strong> 和 <strong>PCA</strong> 在作用上的相似性，具体证明省略。从实验来看，二者在功能上显示出一定的相似性，这强调了预训练好的 self-attention 对于建模各种模态数据的通用性。</p> <h2 id="4-数据集">4. 数据集</h2> <p>使用了多种数据集进行了广泛的任务测试，详见原文。</p> <h2 id="5-实验结果">5. 实验结果</h2> <ul> <li>长期预测：预测长度更长</li> <li>短期预测：预测长度较短</li> <li>零样本预测：未进行微调</li> <li>少样本预测：只用极少量的训练样本进行微调</li> <li>分类</li> <li>异常检测</li> <li>插补</li> </ul> <p>以上七项任务均进行了测试。</p> <h2 id="6-实验环境">6. 实验环境</h2> <p>作者论文中详细列出了计算成本。</p> <h2 id="7-复现">7. 复现</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-promptcast/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-promptcast</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-promptcast/"><![CDATA[<h1 id="promptcast-a-new-prompt-based-learning-paradigm-for-time-series-forecasting">PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting</h1> <p><img src="/pic/promptcast/structure.jpg" alt="PromptCast 模型架构图"/></p> <p>这篇方法太平凡了，简单说下：</p> <h2 id="研究概述">研究概述</h2> <p>这篇论文发表于2022年，尝试首次将语言模型（LM）应用于时间序列（TS）预测任务。尽管方法本身相对基础，但这标志着在该领域的一次新的尝试。</p> <h2 id="主要方法">主要方法</h2> <p>作者采用了一个直接而简单的策略来实施这一新的学习范式。核心方法包括：</p> <ul> <li><strong>Prompt设计</strong>：通过简单的设计修改，将时间序列数据以适合语言模型处理的方式呈现。</li> <li><strong>直接询问</strong>：类似于提问式学习，直接向模型询问未来的时间序列预测，而不进行复杂的转换或额外的预处理步骤。</li> </ul> <p><img src="/pic/promptcast/prompt.jpg" alt="PromptCast 方法示意图"/></p> <p>这种方法的简便性为时间序列预测领域提供了一种全新的途径，虽然它的技术深度和创新性可能不如其他更复杂的方法，但在简化流程和直接性方面具有其独特的价值。</p> <h2 id="结论">结论</h2> <p>尽管“PromptCast”的方法可能看起来较为平凡，但它开创了利用语言模型进行时间序列预测的新领域，为后续的研究提供了基础。</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-tempo/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-tempo</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-tempo/"><![CDATA[<h1 id="tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting">TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>略</p> <h2 id="2-创新点">2. 创新点</h2> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="/pic/tempo/structure.jpg" alt="TEMPO模型架构图"/> 本文采用GPT作为backbone进行时间序列预测，引入提示工程帮助模型更好地适配来自不同领域的非平稳时序数据。时间序列被解耦为趋势项、季节项和残差项，并映射到相应的隐藏空间，构建GPT能够识别的输入。</p> <h3 id="时间序列解耦">时间序列解耦</h3> <ul> <li><strong>趋势项</strong>：通过计算指定滑窗内的均值得到。</li> <li><strong>周期项</strong>：原序列减去趋势项后，使用Loess smoother计算得到。</li> <li><strong>残差项</strong>：原序列减去趋势项和周期项后得到。</li> </ul> <h3 id="模型建模流程">模型建模流程</h3> <p>使用patchTST架构，首先对周期项进行instance normalization，然后进行patching，随后送入embedding进行编码。</p> <p><img src="/pic/tempo/patchtst.jpg" alt="patchTST架构图"/></p> <h3 id="prompt设计">Prompt设计</h3> <p>构建了一个prompt池，池中保留不同键值对，相似的输入时间序列倾向于从集合中检索出同一组提示词。采用得分匹配机制，将原始序列的embedding与键值对中的键进行相似度计算，选择k个最匹配的键，与原始序列的embedding拼接形成最终输入。</p> <h3 id="gpt预测模型">GPT预测模型</h3> <p>使用预训练的GPT-2进行预测，有两种模型形式：一个是将处理后的趋势项、周期项和残差项拼接后输入GPT；另一个是独立地模型趋势项、周期项和残差项。GPT块内部在训练过程中冻结了前馈层，只更新位置嵌入层和层归一化层的梯度。采用LORA进行微调，以适应不同的时间序列分布。输出结果通过反归一化后累加以得到最终的预测值。</p> <h2 id="4-数据集">4. 数据集</h2> <p>还是很经典的那几个</p> <h2 id="5-实验结果">5. 实验结果</h2> <p>效果还不错，具体见原文。</p> <h2 id="6-实验环境">6. 实验环境</h2> <p>未开源，这里方法简单未复现</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-test/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-test</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-test/"><![CDATA[<h1 id="test-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series">TEST: Text Prototype Aligned Embedding to Activate LLM’s Ability for Time Series</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>在LLM+TS的背景下，存在两种主要方法：LLM-for-TS 设计，即训练一个大型模型并针对下游任务进行微调；TS-for-LLM，即将时间序列转换为模型友好的表示格式，使预训练的LLM可以处理时间序列。由于轻量级等限制，本研究依然聚焦于 TS-for-LLM 策略。</p> <h2 id="2-创新点">2. 创新点</h2> <p>本研究没有使用通道独立策略，保留了多变量时间序列（MTS）的信息。引入软提示（soft prompt）避免了微调所需的高昂训练成本。对齐嵌入方法也是一大亮点。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p>本文模型主要分为两步：构建编码器来嵌入TS和创建可以让LLM接收嵌入的TS的prompt。</p> <h3 id="第一步">第一步</h3> <p><img src="/pic/test/1.jpg" alt="示意图"/> 首先使用经典的滑动窗口方法将TS tokenize，即分成长度不定的子序列，每个子序列是一个标记 s（anchor instance）。然后对每个标记 s，采用弱增强（jitter-and-scale strategy）和强增强（permutation-and-jitter strategy）生成正例，并选取与 s 本身无重叠的实例作为负例。构建神经网络作为编码器将 anchor instance 嵌入成一个 M 维的嵌入向量 e，这些 e 就是最初MTS的嵌入token list。</p> <p>接下来是对比学习部分：</p> <ul> <li><strong>Instance-wise contrast</strong>：保证目标anchor instance与其对应的正token instance尽可能相似，与负token instance差异尽可能大。使用了特定的对比损失函数，并采取了防止嵌入空间坍塌的策略。</li> <li><strong>Feature-wise contrast</strong>：打破实例之间的独立性。在嵌入后，每个minibatch中会有一个由B个instance组成的特征矩阵B×M，聚类未知，将列视为特征的软标签，并对相似特征的组进行区分。</li> <li><strong>Text-prototype-aligned contrast learning</strong>：将时序嵌入向量对齐到LLM的文本表示空间，设计了一个对比损失函数，约束向量的相似性和相似的实例在文本空间也有类似的表示。</li> </ul> <h3 id="第二步soft-prompt">第二步：Soft Prompt</h3> <p>这些soft prompt是针对下游具体任务的嵌入，通过LLM输出和ground truth之间的loss来学习。注意，这里的prompt不再是人类语义。</p> <p><img src="/pic/test/structure.jpg" alt="整体架构图"/></p> <h2 id="4-数据集">4. 数据集</h2> <ul> <li>分类：UCR archive中的所有128种单变量时间序列数据集</li> <li>预测：包括天气、交通、电力、ILI 和 ETT 等8个流行的实际应用基准数据集</li> </ul> <h2 id="5-实验结果">5. 实验结果</h2> <p>时序分类和预测性能与常见的baseline相近。</p> <h2 id="6-实验环境">6. 实验环境</h2> <p>20 NVIDIA Tesla V100-SXM2 GPU with CUDA 11.3.</p> <h2 id="7-复现情况">7. 复现情况</h2> <p>本机环境差跑不起来，但是model看完了</p>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://fscdc.github.io/blog/2024/2024-05-19-time-llm/" rel="alternate" type="text/html" title=""/><published>2024-09-02T13:28:35+00:00</published><updated>2024-09-02T13:28:35+00:00</updated><id>https://fscdc.github.io/blog/2024/2024-05-19-time-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/2024-05-19-time-llm/"><![CDATA[<h1 id="time-llm-time-series-forecasting-by-reprogramming-large-language-models">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h1> <h2 id="1-研究背景动机">1. 研究背景/动机</h2> <p>LLM 在 NLP/CV 领域的表现非常优异，具有很强的通用性，在 zero-shot 和 few-shot 任务上也展现出了良好的性能。然而，在时序预测(ST)领域，大多数模型都具有较强的针对性，缺乏通用性。研究显示，LLM 在模式识别和复杂 token 序列理解方面具有良好的鲁棒性。</p> <h2 id="2-创新点">2. 创新点</h2> <p>本研究在时序预测领域首次尝试引入 LLM，采用的是一种轻量级方法，通过添加轻量级调节层，无需修改原有 LLM 参数，避免了昂贵的训练成本。这些调节层能够在少量样本上进行微调，适应当前任务。其中，时序特征的 reprogramming 是一个创新点，具有新颖性和可解释性。</p> <h2 id="3-主要方法">3. 主要方法</h2> <p><img src="/pic/time-llm/structure.jpg" alt="架构图"/> 架构中的 LLM 参数保持不变（Frozen），在其前后各添加了一个可训练的层（Patch Reprogramming 和 Output Projection）。这里采用通道独立策略。</p> <h3 id="input-embedding">Input Embedding</h3> <p>如上图序号 1 和序号 2 所示，时间序列先通过 RevIN 的归一化操作，然后分 patch 进行 embedding。具体数据格式可见： <img src="/pic/time-llm/data.jpg" alt="数据格式"/></p> <h3 id="patch-reprogramming">Patch Reprogramming</h3> <p>数据通过前面的处理后仍为时序数据，因此需要转换为文本格式供 LLM 使用。这里采用了 cross-attention 来对齐不同模态，通过一个 linear 层从 V 个词中抽取 V’ 个 prototypes，减少了处理的复杂性。具体架构如下图所示： <img src="/pic/time-llm/reprogram.jpg" alt="架构细节"/></p> <h3 id="prompt-as-prefix">Prompt-as-Prefix</h3> <p>将时间序列数据集的一些先验信息，以自然语言的形式作为前缀 prompt，并与对齐后的时序特征拼接后输入到 LLM。这可以参照总体架构图的序号 4。一个可能的 prompt 示例： <img src="/pic/time-llm/prompt.jpg" alt="Prompt 示例"/></p> <h3 id="output-projection">Output Projection</h3> <p>丢弃前缀部分，将剩余部分扁平化处理，然后通过线性投影映射到最终结果的格式上。</p> <h2 id="4-数据集">4. 数据集</h2> <ul> <li>长期：ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI</li> <li>短期：M4 benchmark</li> </ul> <h2 id="5-实验结果">5. 实验结果</h2> <ul> <li>长期、短期</li> <li>Few-shot on 10%/5%</li> <li>Zero-shot</li> </ul> <p>整个效果都不错，具体数据可见原论文的Exp部分，这里省略。</p> <h2 id="6-复现情况">6. 复现情况</h2> <p>FINISH</p>]]></content><author><name></name></author></entry><entry><title type="html">Backdoor Attack</title><link href="https://fscdc.github.io/blog/2024/backdoor/" rel="alternate" type="text/html" title="Backdoor Attack"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/backdoor</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/backdoor/"><![CDATA[<p>This Page collects the papers and codes of Backdoor attacks on LLM or TS. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li> <p>Backdoor Learning: A Survey <a href="https://arxiv.org/pdf/2007.08745.pdf">[Paper]</a> <a href="./BL-survey.md">[Note]</a></p> </li> <li> <p>A survey on Large Language Model (LLM) security and privacy: The Good, The Bad, and The Ugly <a href="https://www.sciencedirect.com/science/article/pii/S266729522400014X#sec6">[Paper]</a> <a href="./servey4llmsp.md">[Note]</a></p> </li> <li> <p>A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks <a href="https://arxiv.org/pdf/2308.14367.pdf">[Paper]</a> <a href="./BA4llm.md">[Note]</a></p> </li> <li> <p>Backdoor Attacks on Time Series: A Generative Approach, in <em>arXiv</em> 2022. <a href="https://arxiv.org/pdf/2211.07915.pdf">[Paper]</a></p> </li> <li> <p>Paper List: Awesome Data Poisoning and Backdoor Attacks <a href="https://github.com/penghui-yang/awesome-data-poisoning-and-backdoor-attack">[GitHub]</a></p> </li> </ul> <p>PS: There are more paper and notes in my FEISHU doc, click <a href="https://nankai.feishu.cn/wiki/SCNGw6cpHiWD4xk8hYocqVBrnvg?from=from_copylink">link</a> to veiw(I will gradually transfer them from FEISHU doc to this page.)</p>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Backdoor Attack]]></summary></entry><entry><title type="html">Ai4Bio</title><link href="https://fscdc.github.io/blog/2024/bio/" rel="alternate" type="text/html" title="Ai4Bio"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/bio</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/bio/"><![CDATA[<p>This Page collects the papers and codes of AI4Bio. Additional, I read paper and take notes.</p> <p>LIST:</p> <ul> <li>scGPT: toward building a foundation model for single-cell multi-omics using generative AI <a href="https://www.nature.com/articles/s41592-024-02201-0">[Paper]</a> <a href="./scGPT.md">[Note]</a></li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Ai4Bio]]></summary></entry><entry><title type="html">Efficient LLM</title><link href="https://fscdc.github.io/blog/2024/efficient-llm/" rel="alternate" type="text/html" title="Efficient LLM"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/efficient-llm</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/efficient-llm/"><![CDATA[<p>This Page collects the papers and codes of Efficient AI, Efficient Large Language Models (LLMs). Additional, I read paper and take notes.</p> <p>LIST:</p> <h2 id="-efficient-llm">🦙 Efficient LLM</h2> <h3 id="survey">Survey</h3> <ul> <li>Efficient Large Language Models: A Survey, <a href="https://arxiv.org/abs/2312.03863">Arxiv</a>, <a href="https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey">Github repo</a></li> </ul> <h2 id="books--courses">Books &amp; Courses</h2> <ul> <li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning</a> @MIT by Prof. Song Han (I may update some my learning notes later on my <a href="https://fscdc.github.io/">homepage</a>)</li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Efficient LLM]]></summary></entry><entry><title type="html">Interesting Paper</title><link href="https://fscdc.github.io/blog/2024/interesting/" rel="alternate" type="text/html" title="Interesting Paper"/><published>2024-05-20T00:13:14+00:00</published><updated>2024-05-20T00:13:14+00:00</updated><id>https://fscdc.github.io/blog/2024/interesting</id><content type="html" xml:base="https://fscdc.github.io/blog/2024/interesting/"><![CDATA[<p>This Page collects the papers and codes of which attracted my interests. Additional, I read paper and take notes.</p> <p><em>Keyword: effective LLM, multimodal, cross-discipline, Leverage Learning.</em></p> <p>LIST:</p> <ul> <li> <p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces, in <em>arXiv</em> 2023. <a href="https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf">[Paper]</a></p> </li> <li> <p>Token-Efficient Leverage Learning in Large Language Models, in <em>arXiv</em> 2024. <a href="https://arxiv.org/pdf/2404.00914.pdf">[Paper]</a></p> </li> <li> <p>Heterogeneous Graph Neural Network, in <em>ACM</em> 2019 <a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330961">[Paper]</a> <a href="./het.md">[Note]</a></p> </li> <li> <p>S4 model, <a href="https://arxiv.org/pdf/2111.00396.pdf">[Paper]</a> <a href="./s4.md">[Note]</a></p> </li> </ul>]]></content><author><name></name></author><category term="subdirection"/><category term="summary"/><category term="note"/><summary type="html"><![CDATA[Interesting Paper]]></summary></entry></feed>